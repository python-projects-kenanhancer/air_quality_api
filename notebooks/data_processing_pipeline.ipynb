{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To pass system arguments, uncomment the following cell\n",
    "\n",
    "Change arguments in terms of your requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"data_processing_pipeline.py\",\n",
      "    \"--stage\",\n",
      "    \"development\",\n",
      "    \"--log_group_name\",\n",
      "    \"data_processing_pipeline_development\",\n",
      "    \"--database_url\",\n",
      "    \"postgresql://air_quality_user:ab12cd34@localhost:5433/air_quality_db\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the project root directory to sys.path to enable module imports\n",
    "project_root = Path.cwd().parent.resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from app.schemas.settings import settings\n",
    "\n",
    "\n",
    "stage = settings.stage\n",
    "current_time = datetime.now()\n",
    "\n",
    "\n",
    "sys.argv = [\n",
    "    \"data_processing_pipeline.py\",  # sys.argv[0], script name\n",
    "    \"--stage\",\n",
    "    stage,\n",
    "    \"--log_group_name\",\n",
    "    f\"data_processing_pipeline_{stage}\",\n",
    "    \"--database_url\",\n",
    "    settings.db_url,\n",
    "]\n",
    "\n",
    "print(json.dumps(sys.argv, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from app.utils.arg_utils import get_system_args\n",
    "from app.db.models.air_quality import AirQualityData\n",
    "from app.db.database_manager import DatabaseManager\n",
    "from notebooks.log_utils import LogUtils, log_operation\n",
    "from notebooks.data_utils import (\n",
    "    get_netcdf_file,\n",
    "    process_netcdf_file,\n",
    "    save_processed_data_to_parquet,\n",
    ")\n",
    "from notebooks.errors.no_data_found_error import NoDataFoundError\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.argv = [\n",
      "    \"data_processing_pipeline.py\",\n",
      "    \"--stage\",\n",
      "    \"development\",\n",
      "    \"--log_group_name\",\n",
      "    \"data_processing_pipeline_development\",\n",
      "    \"--database_url\",\n",
      "    \"postgresql://air_quality_user:ab12cd34@localhost:5433/air_quality_db\"\n",
      "]\n",
      "args_dict = {\n",
      "    \"stage\": \"development\",\n",
      "    \"database_url\": \"postgresql://air_quality_user:ab12cd34@localhost:5433/air_quality_db\",\n",
      "    \"log_group_name\": \"data_processing_pipeline_development\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Configure system arguments\n",
    "args = get_system_args()\n",
    "\n",
    "# Convert argparse Namespace to dictionary\n",
    "args_dict = vars(args)\n",
    "\n",
    "print(f\"sys.argv = {json.dumps(sys.argv, indent=4)}\")\n",
    "print(f\"args_dict = {json.dumps(args_dict, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 21:01:44,611 - data_processing_pipeline_development - INFO - Testing log message\n"
     ]
    }
   ],
   "source": [
    "log_utils = LogUtils(stage=args.stage)\n",
    "\n",
    "log_utils.configure_logging()\n",
    "logger = logging.getLogger(args.log_group_name)\n",
    "\n",
    "logger.info(\"Testing log message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load netCDF Data and Export to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "data_dir = Path(\"../data\")\n",
    "processed_data_dir = Path(\"../processed_data\")\n",
    "\n",
    "# Ensure the processed data directory exists\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_data(year: int):\n",
    "    # Locate the netCDF file for the specified year\n",
    "    file_path = log_operation(\n",
    "        \"Locate netCDF File\", lambda: get_netcdf_file(data_dir, year)\n",
    "    )\n",
    "\n",
    "    if file_path:\n",
    "        # Process the netCDF file to get a DataFrame\n",
    "        df = log_operation(\n",
    "            \"Process netCDF File\", lambda: process_netcdf_file(file_path, year)\n",
    "        )\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            raise NoDataFoundError(year)\n",
    "\n",
    "        log_operation(\n",
    "            \"Save processed data to Parquet\",\n",
    "            lambda: save_processed_data_to_parquet(df, processed_data_dir, year),\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No netCDF file found for the year {year}\")\n",
    "\n",
    "\n",
    "year_range = range(1998, 2023)  # Years from 1998 to 2022\n",
    "\n",
    "for year in year_range:\n",
    "    process_data(year)\n",
    "\n",
    "logger.info(\"All years processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Insert of Parquet Data into PostgreSQL with Optimized Processing (It takes hours so if you need all data then uncomment below cell)\n",
    "\n",
    "This script processes large parquet files and inserts their data into a PostgreSQL database in an optimized and efficient manner. The focus is on handling large datasets while managing memory usage and ensuring robust error handling.\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "# Run the bulk insertion process with a batch size of 1000 records\n",
    "load_parquet_to_postgres_bulk(batch_size=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     numeric_cols = df.select_dtypes(include=[\"int\", \"float\"]).columns\n",
    "#     for col in numeric_cols:\n",
    "#         df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # or 'float' based on data\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def insert_records(\n",
    "#     db_session, records: list, file_name: str, row_group: int, batch_num: int\n",
    "# ):\n",
    "#     try:\n",
    "#         db_session.bulk_insert_mappings(AirQualityData, records)\n",
    "#         db_session.commit()\n",
    "#         logger.info(\n",
    "#             f\"Inserted {len(records)} records from {file_name} \"\n",
    "#             f\"row group {row_group + 1} batch {batch_num + 1} into PostgreSQL.\"\n",
    "#         )\n",
    "#     except SQLAlchemyError as e:\n",
    "#         db_session.rollback()\n",
    "#         logger.error(\n",
    "#             f\"SQLAlchemyError inserting records from {file_name} row group {row_group + 1}: {e}\"\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         db_session.rollback()\n",
    "#         logger.error(\n",
    "#             f\"Unexpected error inserting records from {file_name} row group {row_group + 1}: {e}\"\n",
    "#         )\n",
    "\n",
    "\n",
    "# def process_parquet_file(file_path: str, db_session, batch_size: int):\n",
    "#     file_name = os.path.basename(file_path)\n",
    "#     logger.info(f\"Loading {file_name} into PostgreSQL...\")\n",
    "\n",
    "#     try:\n",
    "#         # Open the Parquet file with PyArrow\n",
    "#         parquet_file = pq.ParquetFile(file_path)\n",
    "#         num_row_groups = parquet_file.num_row_groups\n",
    "#         logger.info(f\"{file_name} has {num_row_groups} row groups.\")\n",
    "\n",
    "#         # Iterate over each row group\n",
    "#         for rg in range(num_row_groups):\n",
    "#             table = parquet_file.read_row_group(rg)\n",
    "#             df = table.to_pandas()\n",
    "#             logger.info(\n",
    "#                 f\"Row group {rg + 1}/{num_row_groups} loaded into DataFrame with {len(df)} records.\"\n",
    "#             )\n",
    "\n",
    "#             # Optimize DataFrame memory usage\n",
    "#             df = optimize_dataframe(df)\n",
    "\n",
    "#             # Iterate over DataFrame in smaller batches\n",
    "#             for batch_num, start in enumerate(range(0, len(df), batch_size)):\n",
    "#                 end = start + batch_size\n",
    "#                 batch_df = df.iloc[start:end]\n",
    "#                 records = batch_df.to_dict(orient=\"records\")\n",
    "\n",
    "#                 # Insert the batch into PostgreSQL\n",
    "#                 insert_records(db_session, records, file_name, rg, batch_num)\n",
    "\n",
    "#             # Free up memory\n",
    "#             del df\n",
    "#             gc.collect()\n",
    "\n",
    "#     except SQLAlchemyError as e:\n",
    "#         logger.error(f\"SQLAlchemyError processing {file_name}: {e}\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Unexpected error processing {file_name}: {e}\")\n",
    "\n",
    "\n",
    "# def load_parquet_to_postgres_bulk(batch_size=1000):\n",
    "#     # Initialize Database Manager\n",
    "#     db_manager = DatabaseManager(database_url=args.database_url)\n",
    "#     db_manager.recreate_tables()\n",
    "\n",
    "#     # Define the processed data directory\n",
    "#     processed_data_dir = Path(\"../processed_data\").resolve()\n",
    "\n",
    "#     # Establish a database session using the context manager\n",
    "#     with db_manager.get_db() as db_session:\n",
    "#         # Loop through each processed Parquet file\n",
    "#         for file_name in os.listdir(processed_data_dir):\n",
    "#             if file_name.endswith(\".parquet\"):\n",
    "#                 file_path = os.path.join(processed_data_dir, file_name)\n",
    "#                 process_parquet_file(file_path, db_session, batch_size)\n",
    "\n",
    "#     logger.info(\"All Parquet files have been loaded into PostgreSQL (Bulk Insert).\")\n",
    "\n",
    "\n",
    "# load_parquet_to_postgres_bulk(batch_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Insert of Parquet Data into PostgreSQL with Sampling\n",
    "\n",
    "This script processes large parquet files, samples data, and inserts it into a PostgreSQL database in batches. Below is an overview of the key components and functionality:\n",
    "\n",
    "### Usage\n",
    "\n",
    "You can control the number of records being processed from each parquet file by adjusting the `sample_size` and `batch_size`:\n",
    "\n",
    "```python\n",
    "# Process files with a batch size of 1000 and sample 10,000 records from each parquet file, only for the years 2000-2020\n",
    "load_parquet_to_postgres_bulk(batch_size=1000, sample_size=10000, start_year=2000, end_year=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 21:02:08,055 - root - INFO - All tables dropped successfully.\n",
      "2024-10-12 21:02:08,086 - root - INFO - All tables created successfully.\n",
      "2024-10-12 21:02:08,087 - root - INFO - All tables have been dropped and recreated successfully.\n",
      "2024-10-12 21:02:08,088 - data_processing_pipeline_development - INFO - Loading pm25_processed_2022.parquet into PostgreSQL...\n",
      "2024-10-12 21:02:08,089 - data_processing_pipeline_development - INFO - pm25_processed_2022.parquet has 9 row groups.\n",
      "2024-10-12 21:02:08,619 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:09,883 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 1.\n",
      "2024-10-12 21:02:09,934 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:09,990 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:10,024 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:10,043 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:10,062 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:10,081 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:10,100 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:10,120 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:10,139 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:10,158 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:10,774 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:12,066 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 2.\n",
      "2024-10-12 21:02:12,092 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:12,111 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:12,131 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:12,149 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:12,168 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:12,187 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:12,205 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:12,223 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:12,241 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:12,260 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:12,860 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:14,163 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 3.\n",
      "2024-10-12 21:02:14,187 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:14,206 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:14,224 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:14,243 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:14,262 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:14,280 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:14,298 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:14,317 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:14,335 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:14,353 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:14,954 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:16,284 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 4.\n",
      "2024-10-12 21:02:16,310 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:16,329 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:16,348 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:16,367 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:16,386 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:16,404 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:16,422 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:16,441 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:16,459 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:16,478 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:17,068 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:18,338 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 5.\n",
      "2024-10-12 21:02:18,362 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:18,380 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:18,399 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:18,418 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:18,436 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:18,455 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:18,473 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:18,490 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:18,509 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:18,527 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:19,117 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:20,408 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 6.\n",
      "2024-10-12 21:02:20,432 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:20,451 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:20,470 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:20,489 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:20,508 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:20,526 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:20,545 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:20,564 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:20,584 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:20,602 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:21,189 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:22,479 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 7.\n",
      "2024-10-12 21:02:22,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:22,521 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:22,540 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:22,558 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:22,578 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:22,597 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:22,615 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:22,632 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:22,650 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:22,668 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:23,248 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:24,539 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 8.\n",
      "2024-10-12 21:02:24,562 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:24,580 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:24,597 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:24,615 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:24,634 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:24,652 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:24,670 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:24,688 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:24,706 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:24,758 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:25,327 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:26,606 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2022.parquet row group 9.\n",
      "2024-10-12 21:02:26,630 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:26,648 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:26,666 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:26,684 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:26,703 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:26,721 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:26,739 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:26,757 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:26,775 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:26,794 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2022.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:26,838 - data_processing_pipeline_development - INFO - Skipping pm25_processed_1998.parquet as it is outside the year range.\n",
      "2024-10-12 21:02:26,839 - data_processing_pipeline_development - INFO - Loading pm25_processed_2004.parquet into PostgreSQL...\n",
      "2024-10-12 21:02:26,840 - data_processing_pipeline_development - INFO - pm25_processed_2004.parquet has 9 row groups.\n",
      "2024-10-12 21:02:27,348 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:28,633 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 1.\n",
      "2024-10-12 21:02:28,658 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:28,677 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:28,695 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:28,713 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:28,731 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:28,750 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:28,767 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:28,786 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:28,804 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:28,821 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:29,394 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:30,688 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 2.\n",
      "2024-10-12 21:02:30,710 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:30,729 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:30,746 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:30,764 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:30,782 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:30,799 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:30,817 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:30,835 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:30,853 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:30,871 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:31,432 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:32,730 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 3.\n",
      "2024-10-12 21:02:32,753 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:32,771 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:32,788 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:32,806 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:32,824 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:32,842 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:32,860 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:32,879 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:32,897 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:32,914 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:33,479 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:34,759 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 4.\n",
      "2024-10-12 21:02:34,785 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:34,804 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:34,822 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:34,841 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:34,859 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:34,877 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:34,895 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:34,914 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:34,932 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:34,950 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:35,504 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:36,786 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 5.\n",
      "2024-10-12 21:02:36,810 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:36,828 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:36,845 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:36,863 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:36,881 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:36,900 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:36,918 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:36,936 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:36,954 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:36,972 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:37,530 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:38,840 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 6.\n",
      "2024-10-12 21:02:38,865 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:38,883 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:38,901 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:38,920 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:38,940 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:38,958 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:38,977 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:38,995 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:39,013 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:39,036 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:39,607 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:40,851 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 7.\n",
      "2024-10-12 21:02:40,876 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:40,894 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:40,912 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:40,929 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:40,947 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:40,965 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:40,984 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:41,002 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:41,020 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:41,038 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:41,595 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:42,828 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 8.\n",
      "2024-10-12 21:02:42,851 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:42,868 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:42,886 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:42,904 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:42,921 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:42,939 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:42,957 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:42,975 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:42,993 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:43,011 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:43,559 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:44,851 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2004.parquet row group 9.\n",
      "2024-10-12 21:02:44,872 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:44,890 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:44,908 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:44,927 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:44,945 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:44,964 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:44,982 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:45,001 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:45,019 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:45,038 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2004.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:45,081 - data_processing_pipeline_development - INFO - Loading pm25_processed_2014.parquet into PostgreSQL...\n",
      "2024-10-12 21:02:45,082 - data_processing_pipeline_development - INFO - pm25_processed_2014.parquet has 9 row groups.\n",
      "2024-10-12 21:02:45,599 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:46,866 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 1.\n",
      "2024-10-12 21:02:46,890 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:46,908 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:46,925 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:46,943 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:46,962 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:46,980 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:46,997 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:47,015 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:47,032 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:47,051 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:47,603 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:48,916 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 2.\n",
      "2024-10-12 21:02:48,946 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:48,963 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:48,982 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:48,999 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:49,018 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:49,036 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:49,055 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:49,073 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:49,091 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:49,109 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:49,660 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:50,947 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 3.\n",
      "2024-10-12 21:02:50,972 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:50,989 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:51,007 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:51,026 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:51,044 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:51,062 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:51,080 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:51,097 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:51,115 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:51,135 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:51,688 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:52,968 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 4.\n",
      "2024-10-12 21:02:52,992 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:53,010 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:53,028 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:53,046 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:53,064 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:53,082 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:53,099 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:53,118 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:53,136 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:53,154 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:53,707 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:54,982 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 5.\n",
      "2024-10-12 21:02:55,011 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:55,029 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:55,048 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:55,066 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:55,088 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:55,106 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:55,124 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:55,142 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:55,159 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:55,177 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:55,733 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:57,047 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 6.\n",
      "2024-10-12 21:02:57,071 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:57,089 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:57,107 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:57,126 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:57,144 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:57,163 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:57,182 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:57,201 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:57,221 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:57,240 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:57,821 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:02:59,123 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 7.\n",
      "2024-10-12 21:02:59,148 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:02:59,167 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:02:59,185 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:02:59,203 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:02:59,221 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:02:59,240 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:02:59,259 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:02:59,278 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:02:59,296 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:02:59,314 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:02:59,891 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:01,255 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 8.\n",
      "2024-10-12 21:03:01,281 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:01,299 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:01,318 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:01,335 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:01,353 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:01,376 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:01,394 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:01,412 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:01,430 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:01,451 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:02,012 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:03,339 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2014.parquet row group 9.\n",
      "2024-10-12 21:03:03,365 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:03,383 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:03,403 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:03,422 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:03,485 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:03,516 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:03,537 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:03,560 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:03,580 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:03,600 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2014.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:03,647 - data_processing_pipeline_development - INFO - Loading pm25_processed_2015.parquet into PostgreSQL...\n",
      "2024-10-12 21:03:03,648 - data_processing_pipeline_development - INFO - pm25_processed_2015.parquet has 9 row groups.\n",
      "2024-10-12 21:03:04,169 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:05,433 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 1.\n",
      "2024-10-12 21:03:05,458 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:05,476 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:05,494 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:05,512 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:05,529 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:05,547 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:05,567 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:05,584 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:05,602 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:05,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:06,173 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:07,459 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 2.\n",
      "2024-10-12 21:03:07,483 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:07,501 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:07,519 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:07,537 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:07,568 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:07,607 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:07,624 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:07,642 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:07,660 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:07,677 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:08,237 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:09,530 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 3.\n",
      "2024-10-12 21:03:09,554 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:09,573 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:09,592 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:09,610 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:09,627 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:09,645 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:09,664 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:09,682 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:09,701 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:09,719 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:10,300 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:11,601 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 4.\n",
      "2024-10-12 21:03:11,625 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:11,643 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:11,662 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:11,680 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:11,698 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:11,717 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:11,735 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:11,752 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:11,771 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:11,788 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:12,341 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:13,622 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 5.\n",
      "2024-10-12 21:03:13,648 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:13,665 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:13,684 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:13,702 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:13,720 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:13,737 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:13,754 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:13,771 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:13,788 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:13,806 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:14,358 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:15,675 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 6.\n",
      "2024-10-12 21:03:15,700 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:15,720 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:15,739 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:15,757 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:15,775 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:15,793 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:15,812 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:15,830 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:15,847 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:15,865 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:16,438 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:17,737 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 7.\n",
      "2024-10-12 21:03:17,762 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:17,780 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:17,798 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:17,817 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:17,835 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:17,853 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:17,871 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:17,889 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:17,909 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:17,927 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:18,486 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:19,793 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 8.\n",
      "2024-10-12 21:03:19,815 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:19,834 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:19,854 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:19,873 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:19,892 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:19,911 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:19,930 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:19,949 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:19,967 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:19,986 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:20,584 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:21,905 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2015.parquet row group 9.\n",
      "2024-10-12 21:03:21,930 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:21,947 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:21,965 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:21,982 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:22,000 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:22,018 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:22,036 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:22,053 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:22,070 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:22,087 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2015.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:22,127 - data_processing_pipeline_development - INFO - Loading pm25_processed_2005.parquet into PostgreSQL...\n",
      "2024-10-12 21:03:22,129 - data_processing_pipeline_development - INFO - pm25_processed_2005.parquet has 9 row groups.\n",
      "2024-10-12 21:03:22,627 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:23,931 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 1.\n",
      "2024-10-12 21:03:23,960 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:23,979 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:23,997 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:24,013 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:24,031 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:24,047 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:24,064 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:24,081 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:24,098 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:24,115 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:24,662 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:25,945 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 2.\n",
      "2024-10-12 21:03:25,969 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:25,987 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:26,005 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:26,022 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:26,039 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:26,056 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:26,074 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:26,091 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:26,109 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:26,127 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:26,683 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:27,959 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 3.\n",
      "2024-10-12 21:03:27,982 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:28,000 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:28,018 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:28,035 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:28,053 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:28,072 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:28,090 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:28,108 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:28,125 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:28,143 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:28,689 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:30,011 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 4.\n",
      "2024-10-12 21:03:30,032 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:30,051 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:30,069 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:30,088 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:30,107 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:30,124 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:30,142 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:30,159 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:30,178 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:30,195 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:30,750 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:32,071 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 5.\n",
      "2024-10-12 21:03:32,096 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:32,113 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:32,131 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:32,148 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:32,165 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:32,182 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:32,199 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:32,217 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:32,234 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:32,251 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:32,797 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:34,093 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 6.\n",
      "2024-10-12 21:03:34,125 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:34,143 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:34,162 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:34,181 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:34,199 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:34,218 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:34,236 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:34,254 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:34,273 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:34,291 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:34,857 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:36,247 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 7.\n",
      "2024-10-12 21:03:36,270 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:36,289 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:36,308 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:36,326 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:36,345 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:36,364 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:36,382 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:36,401 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:36,420 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:36,439 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:37,001 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:38,280 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 8.\n",
      "2024-10-12 21:03:38,303 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:38,321 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:38,339 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:38,357 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:38,375 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:38,393 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:38,411 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:38,429 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:38,446 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:38,467 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:39,021 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:40,260 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2005.parquet row group 9.\n",
      "2024-10-12 21:03:40,286 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:40,304 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:40,322 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:40,340 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:40,359 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:40,377 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:40,395 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:40,413 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:40,431 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:40,448 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2005.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:40,489 - data_processing_pipeline_development - INFO - Skipping pm25_processed_1999.parquet as it is outside the year range.\n",
      "2024-10-12 21:03:40,490 - data_processing_pipeline_development - INFO - Loading pm25_processed_2007.parquet into PostgreSQL...\n",
      "2024-10-12 21:03:40,490 - data_processing_pipeline_development - INFO - pm25_processed_2007.parquet has 9 row groups.\n",
      "2024-10-12 21:03:40,999 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:42,287 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 1.\n",
      "2024-10-12 21:03:42,309 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:42,326 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:42,346 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:42,364 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:42,381 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:42,402 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:42,419 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:42,437 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:42,454 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:42,471 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:43,022 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:44,334 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 2.\n",
      "2024-10-12 21:03:44,358 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:44,376 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:44,394 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:44,412 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:44,429 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:44,447 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:44,464 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:44,482 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:44,499 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:44,516 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:45,068 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:46,343 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 3.\n",
      "2024-10-12 21:03:46,369 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:46,386 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:46,404 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:46,421 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:46,438 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:46,456 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:46,473 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:46,490 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:46,508 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:46,526 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:47,084 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:48,359 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 4.\n",
      "2024-10-12 21:03:48,382 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:48,399 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:48,417 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:48,435 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:48,452 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:48,470 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:48,491 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:48,509 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:48,526 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:48,543 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:49,101 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:50,409 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 5.\n",
      "2024-10-12 21:03:50,434 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:50,451 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:50,469 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:50,486 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:50,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:50,520 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:50,537 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:50,554 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:50,571 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:50,588 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:51,154 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:52,424 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 6.\n",
      "2024-10-12 21:03:52,477 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:52,495 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:52,513 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:52,530 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:52,548 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:52,565 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:52,582 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:52,600 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:52,617 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:52,635 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:53,197 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:54,485 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 7.\n",
      "2024-10-12 21:03:54,509 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:54,526 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:54,544 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:54,561 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:54,578 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:54,600 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:54,617 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:54,635 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:54,653 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:54,670 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:55,228 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:56,536 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 8.\n",
      "2024-10-12 21:03:56,561 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:56,581 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:56,601 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:56,622 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:56,641 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:56,659 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:56,678 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:56,697 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:56,715 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:56,733 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:57,296 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:03:58,549 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2007.parquet row group 9.\n",
      "2024-10-12 21:03:58,573 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:03:58,591 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:03:58,609 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:03:58,627 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:03:58,645 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:03:58,663 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:03:58,680 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:03:58,697 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:03:58,715 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:03:58,732 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2007.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:03:58,772 - data_processing_pipeline_development - INFO - Loading pm25_processed_2017.parquet into PostgreSQL...\n",
      "2024-10-12 21:03:58,773 - data_processing_pipeline_development - INFO - pm25_processed_2017.parquet has 9 row groups.\n",
      "2024-10-12 21:03:59,277 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:00,572 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 1.\n",
      "2024-10-12 21:04:00,598 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:00,616 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:00,643 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:00,661 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:00,679 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:00,696 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:00,713 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:00,731 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:00,749 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:00,767 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:01,333 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:02,633 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 2.\n",
      "2024-10-12 21:04:02,658 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:02,677 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:02,694 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:02,712 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:02,729 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:02,747 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:02,765 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:02,783 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:02,802 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:02,821 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:03,387 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:04,728 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 3.\n",
      "2024-10-12 21:04:04,758 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:04,777 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:04,795 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:04,813 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:04,832 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:04,849 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:04,868 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:04,886 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:04,904 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:04,922 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:05,494 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:06,931 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 4.\n",
      "2024-10-12 21:04:06,953 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:06,971 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:06,988 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:07,007 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:07,025 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:07,043 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:07,061 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:07,079 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:07,097 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:07,115 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:07,655 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:08,973 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 5.\n",
      "2024-10-12 21:04:08,999 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:09,018 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:09,035 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:09,053 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:09,070 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:09,089 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:09,107 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:09,124 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:09,141 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:09,159 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:09,735 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:11,161 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 6.\n",
      "2024-10-12 21:04:11,185 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:11,202 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:11,220 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:11,237 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:11,255 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:11,273 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:11,291 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:11,308 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:11,326 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:11,344 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:11,915 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:13,275 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 7.\n",
      "2024-10-12 21:04:13,301 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:13,321 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:13,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:13,360 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:13,378 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:13,395 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:13,413 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:13,430 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:13,448 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:13,465 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:14,038 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:15,479 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 8.\n",
      "2024-10-12 21:04:15,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:15,520 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:15,538 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:15,555 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:15,573 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:15,590 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:15,607 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:15,624 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:15,641 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:15,658 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:16,240 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:17,639 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2017.parquet row group 9.\n",
      "2024-10-12 21:04:17,666 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:17,684 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:17,742 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:17,769 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:17,789 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:17,808 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:17,828 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:17,846 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:17,865 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:17,884 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2017.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:17,925 - data_processing_pipeline_development - INFO - Loading pm25_processed_2021.parquet into PostgreSQL...\n",
      "2024-10-12 21:04:17,927 - data_processing_pipeline_development - INFO - pm25_processed_2021.parquet has 9 row groups.\n",
      "2024-10-12 21:04:18,504 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:19,881 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 1.\n",
      "2024-10-12 21:04:19,906 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:19,923 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:19,940 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:19,957 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:19,973 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:19,990 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:20,007 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:20,024 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:20,040 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:20,058 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:20,633 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:22,064 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 2.\n",
      "2024-10-12 21:04:22,093 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:22,111 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:22,129 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:22,148 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:22,166 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:22,184 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:22,203 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:22,222 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:22,241 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:22,259 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:22,826 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:24,245 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 3.\n",
      "2024-10-12 21:04:24,270 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:24,288 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:24,306 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:24,323 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:24,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:24,360 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:24,379 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:24,396 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:24,414 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:24,431 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:24,989 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:26,438 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 4.\n",
      "2024-10-12 21:04:26,463 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:26,481 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:26,500 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:26,518 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:26,536 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:26,554 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:26,572 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:26,590 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:26,608 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:26,626 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:27,201 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:28,580 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 5.\n",
      "2024-10-12 21:04:28,605 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:28,624 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:28,642 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:28,659 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:28,677 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:28,694 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:28,711 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:28,730 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:28,749 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:28,767 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:29,327 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:30,764 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 6.\n",
      "2024-10-12 21:04:30,785 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:30,802 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:30,820 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:30,839 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:30,857 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:30,874 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:30,891 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:30,908 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:30,926 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:30,944 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:31,515 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:33,049 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 7.\n",
      "2024-10-12 21:04:33,073 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:33,091 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:33,110 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:33,129 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:33,146 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:33,165 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:33,184 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:33,203 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:33,222 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:33,240 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:33,859 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:35,144 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 8.\n",
      "2024-10-12 21:04:35,177 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:35,196 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:35,214 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:35,233 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:35,251 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:35,269 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:35,288 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:35,306 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:35,325 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:35,343 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:35,921 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:37,241 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2021.parquet row group 9.\n",
      "2024-10-12 21:04:37,266 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:37,286 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:37,305 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:37,324 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:37,345 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:37,364 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:37,383 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:37,407 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:37,426 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:37,444 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2021.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:37,490 - data_processing_pipeline_development - INFO - Loading pm25_processed_2020.parquet into PostgreSQL...\n",
      "2024-10-12 21:04:37,492 - data_processing_pipeline_development - INFO - pm25_processed_2020.parquet has 9 row groups.\n",
      "2024-10-12 21:04:38,010 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:39,296 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 1.\n",
      "2024-10-12 21:04:39,321 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:39,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:39,360 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:39,380 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:39,398 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:39,417 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:39,436 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:39,455 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:39,475 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:39,493 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:40,108 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:41,429 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 2.\n",
      "2024-10-12 21:04:41,455 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:41,474 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:41,494 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:41,513 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:41,531 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:41,549 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:41,567 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:41,586 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:41,604 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:41,622 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:42,245 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:43,577 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 3.\n",
      "2024-10-12 21:04:43,600 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:43,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:43,637 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:43,655 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:43,673 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:43,692 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:43,710 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:43,729 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:43,748 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:43,767 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:44,394 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:45,757 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 4.\n",
      "2024-10-12 21:04:45,782 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:45,801 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:45,820 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:45,838 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:45,856 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:45,875 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:45,894 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:45,913 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:45,932 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:45,951 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:46,514 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:47,844 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 5.\n",
      "2024-10-12 21:04:47,870 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:47,889 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:47,908 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:47,928 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:47,947 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:47,965 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:47,985 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:48,006 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:48,027 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:48,047 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:48,625 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:49,936 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 6.\n",
      "2024-10-12 21:04:49,962 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:49,981 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:49,998 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:50,017 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:50,036 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:50,054 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:50,073 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:50,091 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:50,111 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:50,130 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:50,743 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:52,067 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 7.\n",
      "2024-10-12 21:04:52,091 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:52,110 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:52,129 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:52,147 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:52,166 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:52,184 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:52,203 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:52,221 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:52,240 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:52,258 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:52,820 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:54,294 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 8.\n",
      "2024-10-12 21:04:54,320 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:54,343 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:54,361 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:54,380 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:54,398 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:54,418 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:54,436 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:54,455 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:54,473 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:54,490 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:55,056 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:56,358 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2020.parquet row group 9.\n",
      "2024-10-12 21:04:56,383 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:56,401 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:56,420 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:56,440 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:56,458 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:56,481 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:56,500 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:56,519 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:56,537 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:56,556 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2020.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:56,599 - data_processing_pipeline_development - INFO - Loading pm25_processed_2016.parquet into PostgreSQL...\n",
      "2024-10-12 21:04:56,600 - data_processing_pipeline_development - INFO - pm25_processed_2016.parquet has 9 row groups.\n",
      "2024-10-12 21:04:57,110 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:04:58,408 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 1.\n",
      "2024-10-12 21:04:58,434 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:04:58,454 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:04:58,473 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:04:58,492 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:04:58,512 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:04:58,530 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:04:58,549 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:04:58,571 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:04:58,589 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:04:58,608 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:04:59,178 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:00,516 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 2.\n",
      "2024-10-12 21:05:00,540 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:00,559 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:00,578 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:00,598 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:00,617 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:00,636 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:00,655 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:00,732 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:00,759 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:00,778 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:01,354 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:02,706 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 3.\n",
      "2024-10-12 21:05:02,730 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:02,748 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:02,767 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:02,786 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:02,805 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:02,823 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:02,886 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:02,905 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:02,923 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:02,942 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:03,503 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:04,861 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 4.\n",
      "2024-10-12 21:05:04,882 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:04,900 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:04,918 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:04,936 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:04,955 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:04,973 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:04,992 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:05,011 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:05,029 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:05,047 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:05,601 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:06,905 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 5.\n",
      "2024-10-12 21:05:06,936 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:06,955 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:06,973 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:06,990 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:07,009 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:07,027 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:07,045 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:07,062 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:07,080 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:07,097 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:07,659 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:08,966 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 6.\n",
      "2024-10-12 21:05:08,990 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:09,007 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:09,029 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:09,047 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:09,065 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:09,082 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:09,101 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:09,120 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:09,137 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:09,154 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:09,713 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:11,014 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 7.\n",
      "2024-10-12 21:05:11,040 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:11,057 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:11,075 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:11,093 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:11,115 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:11,133 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:11,150 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:11,168 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:11,186 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:11,204 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:11,759 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:13,032 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 8.\n",
      "2024-10-12 21:05:13,056 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:13,075 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:13,093 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:13,111 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:13,129 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:13,150 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:13,168 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:13,187 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:13,204 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:13,222 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:13,784 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:15,115 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2016.parquet row group 9.\n",
      "2024-10-12 21:05:15,138 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:15,158 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:15,176 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:15,194 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:15,211 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:15,230 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:15,251 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:15,270 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:15,288 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:15,307 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2016.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:15,350 - data_processing_pipeline_development - INFO - Loading pm25_processed_2006.parquet into PostgreSQL...\n",
      "2024-10-12 21:05:15,351 - data_processing_pipeline_development - INFO - pm25_processed_2006.parquet has 9 row groups.\n",
      "2024-10-12 21:05:15,864 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:17,152 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 1.\n",
      "2024-10-12 21:05:17,176 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:17,194 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:17,212 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:17,230 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:17,248 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:17,266 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:17,284 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:17,306 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:17,324 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:17,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:17,902 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:19,220 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 2.\n",
      "2024-10-12 21:05:19,242 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:19,259 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:19,277 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:19,297 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:19,314 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:19,332 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:19,350 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:19,368 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:19,389 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:19,407 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:19,963 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:21,261 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 3.\n",
      "2024-10-12 21:05:21,284 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:21,302 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:21,319 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:21,337 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:21,354 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:21,371 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:21,390 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:21,411 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:21,434 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:21,452 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:22,017 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:23,338 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 4.\n",
      "2024-10-12 21:05:23,362 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:23,379 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:23,397 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:23,415 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:23,433 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:23,451 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:23,469 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:23,487 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:23,509 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:23,528 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:24,093 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:25,411 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 5.\n",
      "2024-10-12 21:05:25,434 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:25,452 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:25,470 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:25,488 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:25,505 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:25,522 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:25,540 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:25,558 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:25,580 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:25,598 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:26,164 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:27,497 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 6.\n",
      "2024-10-12 21:05:27,521 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:27,540 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:27,559 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:27,579 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:27,598 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:27,617 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:27,635 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:27,653 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:27,701 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:27,728 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:28,295 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:29,612 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 7.\n",
      "2024-10-12 21:05:29,639 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:29,657 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:29,675 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:29,693 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:29,710 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:29,728 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:29,745 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:29,763 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:29,785 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:29,803 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:30,369 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:31,739 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 8.\n",
      "2024-10-12 21:05:31,762 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:31,780 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:31,799 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:31,817 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:31,835 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:31,852 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:31,870 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:31,887 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:31,909 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:31,927 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:32,507 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:33,833 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2006.parquet row group 9.\n",
      "2024-10-12 21:05:33,856 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:33,874 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:33,893 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:33,910 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:33,929 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:33,946 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:33,964 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:33,983 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:34,000 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:34,022 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2006.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:34,062 - data_processing_pipeline_development - INFO - Loading pm25_processed_2003.parquet into PostgreSQL...\n",
      "2024-10-12 21:05:34,063 - data_processing_pipeline_development - INFO - pm25_processed_2003.parquet has 9 row groups.\n",
      "2024-10-12 21:05:34,595 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:35,965 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 1.\n",
      "2024-10-12 21:05:35,990 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:36,008 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:36,027 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:36,046 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:36,064 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:36,082 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:36,100 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:36,118 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:36,139 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:36,157 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:36,716 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:38,022 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 2.\n",
      "2024-10-12 21:05:38,051 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:38,070 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:38,088 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:38,105 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:38,123 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:38,141 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:38,159 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:38,180 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:38,197 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:38,215 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:38,776 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:40,172 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 3.\n",
      "2024-10-12 21:05:40,197 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:40,215 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:40,233 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:40,251 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:40,268 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:40,286 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:40,303 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:40,325 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:40,344 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:40,361 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:40,932 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:42,236 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 4.\n",
      "2024-10-12 21:05:42,263 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:42,281 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:42,300 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:42,319 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:42,337 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:42,354 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:42,376 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:42,394 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:42,411 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:42,428 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:43,036 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:44,433 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 5.\n",
      "2024-10-12 21:05:44,459 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:44,477 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:44,494 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:44,512 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:44,530 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:44,551 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:44,569 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:44,587 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:44,604 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:44,621 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:45,191 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:46,543 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 6.\n",
      "2024-10-12 21:05:46,566 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:46,584 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:46,601 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:46,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:46,641 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:46,658 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:46,675 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:46,693 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:46,711 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:46,729 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:47,316 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:48,641 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 7.\n",
      "2024-10-12 21:05:48,668 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:48,686 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:48,704 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:48,725 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:48,743 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:48,764 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:48,781 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:48,799 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:48,818 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:48,836 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:49,425 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:50,711 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 8.\n",
      "2024-10-12 21:05:50,732 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:50,750 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:50,768 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:50,789 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:50,807 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:50,825 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:50,843 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:50,861 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:50,879 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:50,897 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:51,462 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:52,737 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2003.parquet row group 9.\n",
      "2024-10-12 21:05:52,762 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:52,781 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:52,805 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:52,824 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:52,843 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:52,862 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:52,881 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:52,899 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:52,917 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:52,937 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2003.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:52,987 - data_processing_pipeline_development - INFO - Loading pm25_processed_2013.parquet into PostgreSQL...\n",
      "2024-10-12 21:05:52,990 - data_processing_pipeline_development - INFO - pm25_processed_2013.parquet has 9 row groups.\n",
      "2024-10-12 21:05:53,537 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:54,814 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 1.\n",
      "2024-10-12 21:05:54,838 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:54,856 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:54,875 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:54,893 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:54,912 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:54,931 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:54,950 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:54,968 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:54,986 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:55,005 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:55,573 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:56,862 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 2.\n",
      "2024-10-12 21:05:56,886 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:56,904 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:56,923 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:56,941 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:56,959 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:56,978 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:56,996 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:57,013 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:57,031 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:57,050 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:57,607 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:05:58,950 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 3.\n",
      "2024-10-12 21:05:58,975 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:05:58,994 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:05:59,013 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:05:59,032 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:05:59,051 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:05:59,069 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:05:59,088 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:05:59,108 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:05:59,130 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:05:59,149 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:05:59,727 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:01,029 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 4.\n",
      "2024-10-12 21:06:01,058 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:01,076 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:01,094 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:01,112 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:01,129 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:01,147 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:01,164 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:01,182 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:01,199 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:01,217 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:01,766 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:03,100 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 5.\n",
      "2024-10-12 21:06:03,124 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:03,142 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:03,160 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:03,178 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:03,195 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:03,213 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:03,231 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:03,249 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:03,267 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:03,290 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:03,894 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:05,163 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 6.\n",
      "2024-10-12 21:06:05,188 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:05,207 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:05,225 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:05,243 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:05,261 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:05,280 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:05,298 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:05,316 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:05,335 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:05,352 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:05,906 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:07,221 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 7.\n",
      "2024-10-12 21:06:07,244 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:07,262 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:07,280 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:07,298 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:07,322 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:07,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:07,358 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:07,376 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:07,394 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:07,411 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:07,980 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:09,254 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 8.\n",
      "2024-10-12 21:06:09,279 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:09,298 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:09,317 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:09,337 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:09,355 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:09,375 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:09,393 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:09,412 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:09,430 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:09,449 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:10,027 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:11,269 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2013.parquet row group 9.\n",
      "2024-10-12 21:06:11,293 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:11,311 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:11,329 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:11,348 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:11,367 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:11,385 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:11,404 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:11,422 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:11,441 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:11,460 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2013.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:11,503 - data_processing_pipeline_development - INFO - Loading pm25_processed_2012.parquet into PostgreSQL...\n",
      "2024-10-12 21:06:11,504 - data_processing_pipeline_development - INFO - pm25_processed_2012.parquet has 9 row groups.\n",
      "2024-10-12 21:06:12,007 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:13,329 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 1.\n",
      "2024-10-12 21:06:13,359 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:13,377 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:13,395 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:13,413 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:13,430 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:13,447 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:13,464 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:13,482 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:13,499 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:13,517 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:14,061 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:15,309 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 2.\n",
      "2024-10-12 21:06:15,332 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:15,349 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:15,367 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:15,385 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:15,403 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:15,421 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:15,439 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:15,456 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:15,474 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:15,491 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:16,037 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:17,323 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 3.\n",
      "2024-10-12 21:06:17,346 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:17,363 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:17,381 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:17,398 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:17,415 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:17,433 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:17,450 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:17,468 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:17,485 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:17,502 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:18,050 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:19,332 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 4.\n",
      "2024-10-12 21:06:19,355 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:19,372 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:19,391 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:19,409 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:19,426 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:19,443 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:19,461 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:19,482 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:19,501 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:19,518 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:20,066 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:21,364 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 5.\n",
      "2024-10-12 21:06:21,388 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:21,407 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:21,425 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:21,442 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:21,460 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:21,478 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:21,495 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:21,513 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:21,530 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:21,548 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:22,101 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:23,398 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 6.\n",
      "2024-10-12 21:06:23,422 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:23,439 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:23,456 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:23,480 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:23,499 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:23,518 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:23,535 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:23,553 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:23,570 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:23,588 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:24,145 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:25,447 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 7.\n",
      "2024-10-12 21:06:25,470 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:25,488 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:25,507 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:25,525 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:25,542 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:25,560 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:25,577 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:25,594 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:25,613 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:25,632 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:26,188 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:27,471 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 8.\n",
      "2024-10-12 21:06:27,496 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:27,514 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:27,532 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:27,550 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:27,567 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:27,584 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:27,601 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:27,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:27,637 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:27,654 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:28,203 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:29,475 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2012.parquet row group 9.\n",
      "2024-10-12 21:06:29,501 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:29,523 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:29,542 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:29,560 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:29,578 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:29,596 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:29,614 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:29,631 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:29,649 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:29,667 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2012.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:29,707 - data_processing_pipeline_development - INFO - Loading pm25_processed_2002.parquet into PostgreSQL...\n",
      "2024-10-12 21:06:29,708 - data_processing_pipeline_development - INFO - pm25_processed_2002.parquet has 9 row groups.\n",
      "2024-10-12 21:06:30,212 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:31,504 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 1.\n",
      "2024-10-12 21:06:31,526 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:31,543 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:31,561 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:31,579 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:31,596 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:31,613 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:31,630 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:31,647 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:31,665 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:31,682 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:32,225 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:33,530 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 2.\n",
      "2024-10-12 21:06:33,554 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:33,571 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:33,588 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:33,606 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:33,624 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:33,641 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:33,658 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:33,676 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:33,693 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:33,710 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:34,260 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:35,537 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 3.\n",
      "2024-10-12 21:06:35,560 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:35,578 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:35,597 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:35,615 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:35,634 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:35,652 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:35,669 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:35,686 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:35,703 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:35,722 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:36,273 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:37,551 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 4.\n",
      "2024-10-12 21:06:37,574 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:37,592 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:37,610 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:37,628 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:37,645 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:37,667 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:37,686 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:37,703 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:37,720 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:37,737 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:38,290 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:39,555 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 5.\n",
      "2024-10-12 21:06:39,580 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:39,597 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:39,614 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:39,632 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:39,650 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:39,669 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:39,687 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:39,705 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:39,723 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:39,741 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:40,297 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:41,579 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 6.\n",
      "2024-10-12 21:06:41,602 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:41,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:41,637 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:41,654 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:41,672 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:41,727 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:41,744 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:41,762 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:41,779 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:41,796 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:42,350 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:43,631 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 7.\n",
      "2024-10-12 21:06:43,655 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:43,672 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:43,690 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:43,708 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:43,725 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:43,742 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:43,759 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:43,778 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:43,796 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:43,814 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:44,374 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:45,652 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 8.\n",
      "2024-10-12 21:06:45,677 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:45,697 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:45,715 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:45,734 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:45,752 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:45,770 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:45,787 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:45,805 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:45,823 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:45,841 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:46,394 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:47,673 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2002.parquet row group 9.\n",
      "2024-10-12 21:06:47,700 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:47,719 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:47,737 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:47,754 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:47,772 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:47,791 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:47,810 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:47,828 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:47,846 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:47,864 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2002.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:47,906 - data_processing_pipeline_development - INFO - Loading pm25_processed_2019.parquet into PostgreSQL...\n",
      "2024-10-12 21:06:47,907 - data_processing_pipeline_development - INFO - pm25_processed_2019.parquet has 9 row groups.\n",
      "2024-10-12 21:06:48,412 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:49,679 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 1.\n",
      "2024-10-12 21:06:49,702 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:49,720 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:49,739 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:49,759 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:49,776 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:49,794 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:49,812 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:49,830 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:49,847 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:49,864 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:50,417 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:51,697 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 2.\n",
      "2024-10-12 21:06:51,721 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:51,739 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:51,758 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:51,776 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:51,793 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:51,811 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:51,828 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:51,845 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:51,863 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:51,880 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:52,433 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:53,706 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 3.\n",
      "2024-10-12 21:06:53,733 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:53,751 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:53,769 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:53,789 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:53,807 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:53,825 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:53,844 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:53,862 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:53,880 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:53,897 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:54,446 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:55,759 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 4.\n",
      "2024-10-12 21:06:55,782 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:55,800 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:55,818 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:55,839 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:55,857 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:55,875 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:55,893 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:55,911 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:55,929 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:55,946 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:56,495 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:57,767 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 5.\n",
      "2024-10-12 21:06:57,790 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:57,808 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:57,828 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:57,846 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:57,863 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:57,881 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:57,898 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:57,916 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:57,934 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:57,952 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:06:58,503 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:06:59,768 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 6.\n",
      "2024-10-12 21:06:59,791 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:06:59,809 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:06:59,826 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:06:59,845 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:06:59,863 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:06:59,880 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:06:59,898 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:06:59,915 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:06:59,932 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:06:59,949 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:00,504 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:01,791 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 7.\n",
      "2024-10-12 21:07:01,817 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:01,835 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:01,854 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:01,874 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:01,893 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:01,910 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:01,928 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:01,945 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:01,963 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:01,980 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:02,537 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:03,833 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 8.\n",
      "2024-10-12 21:07:03,856 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:03,873 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:03,891 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:03,909 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:03,927 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:03,944 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:03,995 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:04,013 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:04,031 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:04,048 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:04,597 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:05,866 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2019.parquet row group 9.\n",
      "2024-10-12 21:07:05,891 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:05,909 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:05,926 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:05,944 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:05,961 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:05,978 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:05,995 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:06,012 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:06,030 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:06,049 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2019.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:06,090 - data_processing_pipeline_development - INFO - Loading pm25_processed_2009.parquet into PostgreSQL...\n",
      "2024-10-12 21:07:06,091 - data_processing_pipeline_development - INFO - pm25_processed_2009.parquet has 9 row groups.\n",
      "2024-10-12 21:07:06,594 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:07,886 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 1.\n",
      "2024-10-12 21:07:07,909 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:07,926 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:07,944 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:07,961 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:07,978 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:07,994 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:08,014 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:08,031 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:08,049 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:08,066 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:08,611 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:09,885 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 2.\n",
      "2024-10-12 21:07:09,916 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:09,934 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:09,951 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:09,969 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:09,987 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:10,005 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:10,023 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:10,041 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:10,060 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:10,078 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:10,635 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:11,962 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 3.\n",
      "2024-10-12 21:07:11,984 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:12,003 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:12,021 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:12,039 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:12,057 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:12,075 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:12,093 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:12,111 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:12,129 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:12,146 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:12,701 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:13,983 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 4.\n",
      "2024-10-12 21:07:14,009 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:14,028 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:14,045 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:14,063 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:14,082 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:14,101 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:14,120 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:14,138 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:14,156 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:14,174 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:14,725 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:16,021 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 5.\n",
      "2024-10-12 21:07:16,047 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:16,065 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:16,082 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:16,099 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:16,116 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:16,167 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:16,186 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:16,204 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:16,221 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:16,238 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:16,792 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:18,072 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 6.\n",
      "2024-10-12 21:07:18,101 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:18,120 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:18,137 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:18,155 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:18,172 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:18,189 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:18,207 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:18,224 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:18,243 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:18,260 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:18,814 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:20,094 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 7.\n",
      "2024-10-12 21:07:20,117 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:20,135 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:20,152 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:20,169 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:20,188 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:20,206 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:20,224 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:20,242 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:20,260 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:20,278 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:20,845 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:22,127 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 8.\n",
      "2024-10-12 21:07:22,150 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:22,169 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:22,186 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:22,204 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:22,222 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:22,240 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:22,258 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:22,276 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:22,293 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:22,310 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:22,857 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:24,134 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2009.parquet row group 9.\n",
      "2024-10-12 21:07:24,163 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:24,181 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:24,199 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:24,217 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:24,236 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:24,253 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:24,272 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:24,290 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:24,308 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:24,325 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2009.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:24,365 - data_processing_pipeline_development - INFO - Loading pm25_processed_2000.parquet into PostgreSQL...\n",
      "2024-10-12 21:07:24,366 - data_processing_pipeline_development - INFO - pm25_processed_2000.parquet has 9 row groups.\n",
      "2024-10-12 21:07:24,870 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:26,179 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 1.\n",
      "2024-10-12 21:07:26,203 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:26,221 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:26,239 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:26,256 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:26,273 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:26,290 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:26,307 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:26,324 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:26,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:26,358 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:26,902 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:28,175 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 2.\n",
      "2024-10-12 21:07:28,199 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:28,217 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:28,235 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:28,252 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:28,270 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:28,287 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:28,304 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:28,321 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:28,337 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:28,354 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:28,899 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:30,182 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 3.\n",
      "2024-10-12 21:07:30,205 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:30,224 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:30,242 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:30,260 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:30,278 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:30,295 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:30,345 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:30,364 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:30,381 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:30,398 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:30,955 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:32,241 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 4.\n",
      "2024-10-12 21:07:32,265 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:32,283 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:32,302 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:32,319 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:32,336 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:32,353 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:32,370 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:32,387 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:32,405 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:32,422 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:32,969 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:34,236 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 5.\n",
      "2024-10-12 21:07:34,258 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:34,278 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:34,296 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:34,315 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:34,333 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:34,350 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:34,367 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:34,384 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:34,401 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:34,419 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:34,970 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:36,267 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 6.\n",
      "2024-10-12 21:07:36,290 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:36,307 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:36,326 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:36,343 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:36,360 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:36,377 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:36,396 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:36,413 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:36,431 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:36,448 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:37,001 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:38,287 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 7.\n",
      "2024-10-12 21:07:38,310 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:38,328 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:38,345 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:38,363 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:38,380 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:38,397 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:38,414 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:38,432 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:38,450 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:38,466 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:39,019 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:40,295 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 8.\n",
      "2024-10-12 21:07:40,320 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:40,338 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:40,356 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:40,374 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:40,391 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:40,409 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:40,426 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:40,443 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:40,460 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:40,479 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:41,040 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:42,316 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2000.parquet row group 9.\n",
      "2024-10-12 21:07:42,341 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:42,358 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:42,376 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:42,394 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:42,412 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:42,430 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:42,447 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:42,464 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:42,481 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:42,499 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2000.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:42,539 - data_processing_pipeline_development - INFO - Loading pm25_processed_2010.parquet into PostgreSQL...\n",
      "2024-10-12 21:07:42,540 - data_processing_pipeline_development - INFO - pm25_processed_2010.parquet has 9 row groups.\n",
      "2024-10-12 21:07:43,036 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:44,359 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 1.\n",
      "2024-10-12 21:07:44,384 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:44,402 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:44,419 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:44,436 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:44,452 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:44,469 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:44,486 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:44,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:44,521 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:44,538 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:45,085 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:46,375 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 2.\n",
      "2024-10-12 21:07:46,397 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:46,416 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:46,434 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:46,451 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:46,468 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:46,485 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:46,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:46,520 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:46,537 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:46,554 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:47,106 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:48,392 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 3.\n",
      "2024-10-12 21:07:48,417 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:48,434 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:48,452 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:48,469 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:48,486 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:48,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:48,553 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:48,572 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:48,589 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:48,605 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:49,153 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:50,437 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 4.\n",
      "2024-10-12 21:07:50,460 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:50,477 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:50,494 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:50,511 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:50,528 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:50,545 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:50,565 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:50,583 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:50,601 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:50,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:51,171 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:52,457 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 5.\n",
      "2024-10-12 21:07:52,481 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:52,498 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:52,516 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:52,533 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:52,551 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:52,568 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:52,586 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:52,603 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:52,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:52,671 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:53,221 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:54,501 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 6.\n",
      "2024-10-12 21:07:54,528 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:54,546 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:54,563 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:54,582 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:54,599 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:54,616 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:54,633 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:54,652 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:54,670 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:54,687 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:55,229 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:56,510 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 7.\n",
      "2024-10-12 21:07:56,537 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:56,555 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:56,573 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:56,591 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:56,608 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:56,625 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:56,643 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:56,661 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:56,679 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:56,696 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:57,254 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:07:58,575 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 8.\n",
      "2024-10-12 21:07:58,599 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:07:58,619 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:07:58,636 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:07:58,654 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:07:58,672 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:07:58,690 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:07:58,708 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:07:58,725 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:07:58,742 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:07:58,759 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:07:59,313 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:00,603 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2010.parquet row group 9.\n",
      "2024-10-12 21:08:00,626 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:00,644 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:00,663 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:00,681 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:00,700 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:00,726 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:00,743 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:00,761 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:00,778 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:00,796 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2010.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:00,836 - data_processing_pipeline_development - INFO - Loading pm25_processed_2011.parquet into PostgreSQL...\n",
      "2024-10-12 21:08:00,837 - data_processing_pipeline_development - INFO - pm25_processed_2011.parquet has 9 row groups.\n",
      "2024-10-12 21:08:01,338 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:02,626 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 1.\n",
      "2024-10-12 21:08:02,650 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:02,667 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:02,687 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:02,705 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:02,757 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:02,776 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:02,794 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:02,812 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:02,829 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:02,847 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:03,393 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:04,694 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 2.\n",
      "2024-10-12 21:08:04,717 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:04,736 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:04,753 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:04,770 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:04,790 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:04,807 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:04,825 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:04,843 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:04,861 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:04,879 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:05,455 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:06,740 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 3.\n",
      "2024-10-12 21:08:06,765 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:06,784 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:06,803 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:06,821 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:06,840 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:06,859 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:06,879 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:06,899 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:06,978 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:07,000 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:07,571 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:08,845 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 4.\n",
      "2024-10-12 21:08:08,869 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:08,886 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:08,905 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:08,928 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:08,946 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:08,964 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:08,983 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:09,001 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:09,019 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:09,037 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:09,606 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:10,903 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 5.\n",
      "2024-10-12 21:08:10,928 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:10,946 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:10,964 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:10,981 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:10,998 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:11,054 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:11,072 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:11,089 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:11,106 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:11,124 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:11,694 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:12,974 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 6.\n",
      "2024-10-12 21:08:12,999 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:13,019 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:13,037 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:13,056 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:13,074 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:13,092 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:13,110 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:13,133 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:13,151 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:13,169 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:13,727 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:15,013 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 7.\n",
      "2024-10-12 21:08:15,039 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:15,056 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:15,078 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:15,096 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:15,150 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:15,173 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:15,193 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:15,212 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:15,230 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:15,253 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:15,817 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:17,126 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 8.\n",
      "2024-10-12 21:08:17,149 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:17,168 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:17,189 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:17,207 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:17,229 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:17,247 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:17,266 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:17,284 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:17,302 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:17,320 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:17,893 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:19,197 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2011.parquet row group 9.\n",
      "2024-10-12 21:08:19,217 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:19,235 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:19,254 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:19,272 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:19,291 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:19,310 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:19,328 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:19,350 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:19,368 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:19,386 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2011.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:19,428 - data_processing_pipeline_development - INFO - Loading pm25_processed_2001.parquet into PostgreSQL...\n",
      "2024-10-12 21:08:19,429 - data_processing_pipeline_development - INFO - pm25_processed_2001.parquet has 9 row groups.\n",
      "2024-10-12 21:08:19,934 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:21,249 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 1.\n",
      "2024-10-12 21:08:21,272 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:21,290 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:21,311 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:21,329 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:21,348 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:21,366 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:21,385 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:21,402 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:21,420 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:21,441 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:21,997 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:23,295 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 2.\n",
      "2024-10-12 21:08:23,319 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:23,336 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:23,353 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:23,371 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:23,391 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:23,410 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:23,427 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:23,445 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:23,463 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:23,480 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:24,034 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:25,292 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 3.\n",
      "2024-10-12 21:08:25,315 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:25,333 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:25,351 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:25,369 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:25,387 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:25,404 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:25,425 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:25,443 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:25,461 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:25,478 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:26,042 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:27,342 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 4.\n",
      "2024-10-12 21:08:27,366 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:27,383 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:27,401 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:27,418 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:27,436 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:27,454 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:27,476 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:27,494 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:27,511 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:27,529 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:28,091 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:29,328 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 5.\n",
      "2024-10-12 21:08:29,353 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:29,371 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:29,388 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:29,405 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:29,422 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:29,440 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:29,458 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:29,479 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:29,496 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:29,514 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:30,068 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:31,362 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 6.\n",
      "2024-10-12 21:08:31,391 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:31,408 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:31,426 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:31,444 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:31,461 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:31,478 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:31,497 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:31,519 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:31,536 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:31,554 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:32,118 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:33,387 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 7.\n",
      "2024-10-12 21:08:33,414 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:33,432 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:33,450 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:33,468 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:33,485 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:33,502 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:33,520 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:33,541 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:33,559 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:33,577 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:34,148 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:35,451 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 8.\n",
      "2024-10-12 21:08:35,475 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:35,493 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:35,515 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:35,533 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:35,552 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:35,570 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:35,587 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:35,605 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:35,627 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:35,645 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:36,196 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:37,475 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2001.parquet row group 9.\n",
      "2024-10-12 21:08:37,498 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:37,516 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:37,539 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:37,557 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:37,576 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:37,594 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:37,613 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:37,631 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:37,650 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:37,673 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2001.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:37,720 - data_processing_pipeline_development - INFO - Loading pm25_processed_2008.parquet into PostgreSQL...\n",
      "2024-10-12 21:08:37,721 - data_processing_pipeline_development - INFO - pm25_processed_2008.parquet has 9 row groups.\n",
      "2024-10-12 21:08:38,269 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:39,520 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 1.\n",
      "2024-10-12 21:08:39,544 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:39,567 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:39,584 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:39,606 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:39,623 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:39,640 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:39,657 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:39,674 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:39,691 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:39,712 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:40,280 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:41,630 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 2.\n",
      "2024-10-12 21:08:41,654 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:41,671 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:41,688 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:41,710 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:41,728 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:41,745 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:41,762 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:41,779 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:41,796 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:41,817 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:42,377 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:43,729 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 3.\n",
      "2024-10-12 21:08:43,752 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:43,769 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:43,786 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:43,807 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:43,824 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:43,842 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:43,859 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:43,876 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:43,894 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:43,912 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:44,476 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:45,831 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 4.\n",
      "2024-10-12 21:08:45,855 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:45,872 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:45,890 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:45,908 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:45,926 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:45,947 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:45,965 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:45,983 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:46,001 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:46,018 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:46,572 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:47,880 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 5.\n",
      "2024-10-12 21:08:47,904 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:47,921 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:47,938 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:47,956 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:47,974 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:47,994 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:48,012 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:48,029 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:48,046 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:48,064 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:48,624 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:49,931 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 6.\n",
      "2024-10-12 21:08:49,955 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:49,972 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:49,990 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:50,008 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:50,025 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:50,048 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:50,065 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:50,083 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:50,101 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:50,119 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:50,682 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:51,994 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 7.\n",
      "2024-10-12 21:08:52,017 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:52,035 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:52,054 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:52,073 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:52,091 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:52,115 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:52,133 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:52,152 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:52,170 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:52,189 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:52,774 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:54,073 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 8.\n",
      "2024-10-12 21:08:54,096 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:54,115 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:54,133 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:54,151 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:54,170 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:54,193 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:54,212 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:54,230 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:54,249 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:54,267 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:54,829 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:56,092 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2008.parquet row group 9.\n",
      "2024-10-12 21:08:56,117 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:56,135 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:56,156 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:56,174 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:56,193 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:56,211 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:56,234 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:56,253 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:56,271 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:56,290 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2008.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:56,334 - data_processing_pipeline_development - INFO - Loading pm25_processed_2018.parquet into PostgreSQL...\n",
      "2024-10-12 21:08:56,335 - data_processing_pipeline_development - INFO - pm25_processed_2018.parquet has 9 row groups.\n",
      "2024-10-12 21:08:56,837 - data_processing_pipeline_development - INFO - Row group 1/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:08:58,116 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 1.\n",
      "2024-10-12 21:08:58,175 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:08:58,195 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:08:58,213 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:08:58,231 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:08:58,248 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:08:58,266 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:08:58,287 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:08:58,304 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:08:58,321 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:08:58,339 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 1 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:08:58,882 - data_processing_pipeline_development - INFO - Row group 2/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:00,149 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 2.\n",
      "2024-10-12 21:09:00,173 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:00,191 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:00,208 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:00,226 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:00,244 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:00,262 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:00,283 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:00,301 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:00,318 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:00,336 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 2 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:00,892 - data_processing_pipeline_development - INFO - Row group 3/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:02,202 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 3.\n",
      "2024-10-12 21:09:02,226 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:02,244 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:02,261 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:02,279 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:02,298 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:02,315 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:02,337 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:02,354 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:02,372 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:02,390 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 3 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:02,943 - data_processing_pipeline_development - INFO - Row group 4/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:04,223 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 4.\n",
      "2024-10-12 21:09:04,249 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:04,270 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:04,287 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:04,305 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:04,324 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:04,342 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:04,364 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:04,382 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:04,400 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:04,417 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 4 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:04,972 - data_processing_pipeline_development - INFO - Row group 5/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:06,228 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 5.\n",
      "2024-10-12 21:09:06,257 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:06,274 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:06,292 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:06,310 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:06,327 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:06,344 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:06,362 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:06,383 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:06,401 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:06,418 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 5 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:06,970 - data_processing_pipeline_development - INFO - Row group 6/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:08,254 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 6.\n",
      "2024-10-12 21:09:08,282 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:08,300 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:08,318 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:08,336 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:08,353 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:08,371 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:08,388 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:08,410 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:08,428 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:08,446 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 6 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:09,001 - data_processing_pipeline_development - INFO - Row group 7/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:10,266 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 7.\n",
      "2024-10-12 21:09:10,293 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:10,311 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:10,363 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:10,383 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:10,401 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:10,418 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:10,436 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:10,457 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:10,475 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:10,493 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 7 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:11,052 - data_processing_pipeline_development - INFO - Row group 8/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:12,341 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 8.\n",
      "2024-10-12 21:09:12,369 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:12,387 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:12,405 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:12,423 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:12,440 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:12,459 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:12,477 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:12,498 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:12,515 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:12,533 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 8 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:13,084 - data_processing_pipeline_development - INFO - Row group 9/9 loaded into DataFrame with 49200000 records.\n",
      "2024-10-12 21:09:14,367 - data_processing_pipeline_development - INFO - Sampled 10000 records from pm25_processed_2018.parquet row group 9.\n",
      "2024-10-12 21:09:14,394 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 1 into PostgreSQL.\n",
      "2024-10-12 21:09:14,411 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 2 into PostgreSQL.\n",
      "2024-10-12 21:09:14,429 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 3 into PostgreSQL.\n",
      "2024-10-12 21:09:14,447 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 4 into PostgreSQL.\n",
      "2024-10-12 21:09:14,465 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 5 into PostgreSQL.\n",
      "2024-10-12 21:09:14,485 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 6 into PostgreSQL.\n",
      "2024-10-12 21:09:14,503 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 7 into PostgreSQL.\n",
      "2024-10-12 21:09:14,525 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 8 into PostgreSQL.\n",
      "2024-10-12 21:09:14,543 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 9 into PostgreSQL.\n",
      "2024-10-12 21:09:14,561 - data_processing_pipeline_development - INFO - Inserted 1000 records from pm25_processed_2018.parquet row group 9 batch 10 into PostgreSQL.\n",
      "2024-10-12 21:09:14,602 - data_processing_pipeline_development - INFO - All Parquet files within the year range have been loaded into PostgreSQL (Bulk Insert).\n"
     ]
    }
   ],
   "source": [
    "def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    numeric_cols = df.select_dtypes(include=[\"int\", \"float\"]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # or 'float' based on data\n",
    "    return df\n",
    "\n",
    "\n",
    "def insert_records(\n",
    "    db_session, records: list, file_name: str, row_group: int, batch_num: int\n",
    "):\n",
    "    try:\n",
    "        db_session.bulk_insert_mappings(AirQualityData, records)\n",
    "        db_session.commit()\n",
    "        logger.info(\n",
    "            f\"Inserted {len(records)} records from {file_name} \"\n",
    "            f\"row group {row_group + 1} batch {batch_num + 1} into PostgreSQL.\"\n",
    "        )\n",
    "    except SQLAlchemyError as e:\n",
    "        db_session.rollback()\n",
    "        logger.error(\n",
    "            f\"SQLAlchemyError inserting records from {file_name} row group {row_group + 1}: {e}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        db_session.rollback()\n",
    "        logger.error(\n",
    "            f\"Unexpected error inserting records from {file_name} row group {row_group + 1}: {e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def process_parquet_file(file_path: str, db_session, batch_size: int, sample_size: int):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    logger.info(f\"Loading {file_name} into PostgreSQL...\")\n",
    "\n",
    "    try:\n",
    "        # Open the Parquet file with PyArrow\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        num_row_groups = parquet_file.num_row_groups\n",
    "        logger.info(f\"{file_name} has {num_row_groups} row groups.\")\n",
    "\n",
    "        # Iterate over each row group\n",
    "        for rg in range(num_row_groups):\n",
    "            table = parquet_file.read_row_group(rg)\n",
    "            df = table.to_pandas()\n",
    "            logger.info(\n",
    "                f\"Row group {rg + 1}/{num_row_groups} loaded into DataFrame with {len(df)} records.\"\n",
    "            )\n",
    "\n",
    "            # Sample a subset of records if the sample size is smaller than the DataFrame size\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size)\n",
    "                logger.info(\n",
    "                    f\"Sampled {sample_size} records from {file_name} row group {rg + 1}.\"\n",
    "                )\n",
    "\n",
    "            # Optimize DataFrame memory usage\n",
    "            df = optimize_dataframe(df)\n",
    "\n",
    "            # Iterate over DataFrame in smaller batches\n",
    "            for batch_num, start in enumerate(range(0, len(df), batch_size)):\n",
    "                end = start + batch_size\n",
    "                batch_df = df.iloc[start:end]\n",
    "                records = batch_df.to_dict(orient=\"records\")\n",
    "\n",
    "                # Insert the batch into PostgreSQL\n",
    "                insert_records(db_session, records, file_name, rg, batch_num)\n",
    "\n",
    "            # Free up memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(f\"SQLAlchemyError processing {file_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing {file_name}: {e}\")\n",
    "\n",
    "\n",
    "def load_parquet_to_postgres_bulk(\n",
    "    batch_size=1000, sample_size=10000, start_year=1998, end_year=2022\n",
    "):\n",
    "    # Initialize Database Manager\n",
    "    db_manager = DatabaseManager(database_url=args.database_url)\n",
    "    db_manager.recreate_tables()\n",
    "\n",
    "    # Define the processed data directory\n",
    "    processed_data_dir = Path(\"../processed_data\").resolve()\n",
    "\n",
    "    # Establish a database session using the context manager\n",
    "    with db_manager.get_db() as db_session:\n",
    "        # Loop through each processed Parquet file\n",
    "        for file_name in os.listdir(processed_data_dir):\n",
    "            if file_name.endswith(\".parquet\"):\n",
    "                # Extract the year from the file name using regex\n",
    "                match = re.search(r\"\\d{4}\", file_name)\n",
    "                if match:\n",
    "                    file_year = int(match.group(0))\n",
    "\n",
    "                    # Check if the file's year falls within the year range\n",
    "                    if start_year <= file_year <= end_year:\n",
    "                        file_path = os.path.join(processed_data_dir, file_name)\n",
    "                        process_parquet_file(\n",
    "                            file_path, db_session, batch_size, sample_size\n",
    "                        )\n",
    "                    else:\n",
    "                        logger.info(\n",
    "                            f\"Skipping {file_name} as it is outside the year range.\"\n",
    "                        )\n",
    "                else:\n",
    "                    logger.warning(f\"No year found in {file_name}. Skipping file.\")\n",
    "\n",
    "    logger.info(\n",
    "        \"All Parquet files within the year range have been loaded into PostgreSQL (Bulk Insert).\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Load parquet files with a batch size of 1000 and sample 10,000 records per file, only for the years 2000-2020\n",
    "load_parquet_to_postgres_bulk(\n",
    "    batch_size=1000, sample_size=10000, start_year=2000, end_year=2022\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (air_quality_api)",
   "language": "python",
   "name": "air_quality_api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
